"""
RAG (Retrieval-Augmented Generation) Flask åº”ç”¨
ä½¿ç”¨ LangChain + FAISS + OpenAI å®ç°é—®ç­”ç³»ç»Ÿ
ç®€åŒ–ç‰ˆæœ¬ - å¯åŠ¨æ—¶å¼ºåˆ¶åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
"""
import os
from flask import Flask, request, jsonify
from langchain_community.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader

app = Flask(__name__)

# é…ç½®
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
FAISS_INDEX_PATH = "faiss_index"
DATA_FILE = "data.txt"

# å…¨å±€å˜é‡
qa_chain = None


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶"""
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶...")
    sample_data = """äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚äººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªé¢†åŸŸã€‚

æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼ŒMLï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥è¯†åˆ«æ¨¡å¼ã€åšå‡ºé¢„æµ‹å’Œå†³ç­–ã€‚

æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPåº”ç”¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æå’ŒèŠå¤©æœºå™¨äººã€‚

è®¡ç®—æœºè§†è§‰ï¼ˆComputer Visionï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–é«˜çº§ç†è§£ã€‚è®¡ç®—æœºè§†è§‰åº”ç”¨åŒ…æ‹¬äººè„¸è¯†åˆ«ã€è‡ªåŠ¨é©¾é©¶å’ŒåŒ»å­¦å›¾åƒåˆ†æã€‚

ç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰æ˜¯ä¸€ç§å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆã€‚ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ã€‚

å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ å¦‚ä½•åšå‡ºå†³ç­–ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚å¼ºåŒ–å­¦ä¹ è¢«ç”¨äºæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ç­‰é¢†åŸŸã€‚"""
    
    with open(DATA_FILE, "w", encoding="utf-8") as f:
        f.write(sample_data)
    print(f"âœ… ç¤ºä¾‹æ•°æ®å·²å†™å…¥ {DATA_FILE}")


def create_faiss_index():
    """åˆ›å»ºFAISSå‘é‡ç´¢å¼•"""
    print("ğŸ”¨ åˆ›å»º FAISS å‘é‡ç´¢å¼•...")
    
    # ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(DATA_FILE):
        create_sample_data()
    
    # åŠ è½½æ–‡æ¡£
    loader = TextLoader(DATA_FILE, encoding="utf-8")
    documents = loader.load()
    print(f"ğŸ“„ å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    
    # åˆ†å‰²æ–‡æ¡£
    text_splitter = CharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separator="\n\n"
    )
    texts = text_splitter.split_documents(documents)
    print(f"âœ‚ï¸  æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(texts)} ä¸ªå—")
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    vectorstore = FAISS.from_documents(texts, embeddings)
    print("ğŸ§® å‘é‡åµŒå…¥å·²åˆ›å»º")
    
    # ä¿å­˜ç´¢å¼•
    vectorstore.save_local(FAISS_INDEX_PATH)
    print(f"ğŸ’¾ FAISS ç´¢å¼•å·²ä¿å­˜åˆ° {FAISS_INDEX_PATH}")
    
    return vectorstore


def initialize_qa_system():
    """åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ"""
    global qa_chain
    
    print("\n" + "="*50)
    print("ğŸš€ åˆå§‹åŒ– RAG é—®ç­”ç³»ç»Ÿ")
    print("="*50)
    
    # æ£€æŸ¥ API Key
    if not OPENAI_API_KEY:
        raise ValueError("âŒ é”™è¯¯: æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    
    print(f"âœ… OpenAI API Key å·²è®¾ç½® (é•¿åº¦: {len(OPENAI_API_KEY)})")
    
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
    index_file = os.path.join(FAISS_INDEX_PATH, "index.faiss")
    
    if os.path.exists(index_file):
        print(f"ğŸ“¦ åŠ è½½ç°æœ‰ FAISS ç´¢å¼•: {index_file}")
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = FAISS.load_local(
            FAISS_INDEX_PATH,
            embeddings
        )
    else:
        print("ğŸ“¦ FAISS ç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç´¢å¼•...")
        vectorstore = create_faiss_index()
    
    # åˆå§‹åŒ– LLM
    print("ğŸ¤– åˆå§‹åŒ– OpenAI LLM...")
    llm = OpenAI(
        temperature=0.7,
        openai_api_key=OPENAI_API_KEY
    )
    
    # åˆ›å»º QA chain
    print("ğŸ”— åˆ›å»ºæ£€ç´¢é—®ç­”é“¾...")
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True
    )
    
    print("âœ… RAG é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    print("="*50 + "\n")


# Flask è·¯ç”±
@app.route('/')
def home():
    """é¦–é¡µ"""
    return jsonify({
        "status": "ok",
        "message": "RAG é—®ç­”ç³»ç»Ÿè¿è¡Œä¸­",
        "endpoints": {
            "/health": "å¥åº·æ£€æŸ¥",
            "/ask": "é—®ç­”æ¥å£ (POST)",
            "/info": "ç³»ç»Ÿä¿¡æ¯"
        }
    })


@app.route('/health')
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({"status": "ok"})


@app.route('/info')
def info():
    """ç³»ç»Ÿä¿¡æ¯"""
    return jsonify({
        "app": "RAG é—®ç­”ç³»ç»Ÿ",
        "description": "åŸºäº LangChain + FAISS + OpenAI çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ",
        "version": "2.0-simplified",
        "qa_system_ready": qa_chain is not None,
        "endpoints": {
            "/": "é¦–é¡µ",
            "/health": "å¥åº·æ£€æŸ¥",
            "/ask": "é—®ç­”æ¥å£ (POST)",
            "/info": "ç³»ç»Ÿä¿¡æ¯"
        },
        "usage": {
            "method": "POST",
            "endpoint": "/ask",
            "body": {"question": "ä½ çš„é—®é¢˜"},
            "example": {
                "question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
            }
        }
    })


@app.route('/ask', methods=['POST'])
def ask():
    """é—®ç­”ç«¯ç‚¹"""
    try:
        # æ£€æŸ¥ QA ç³»ç»Ÿæ˜¯å¦å°±ç»ª
        if qa_chain is None:
            return jsonify({
                "error": "QA ç³»ç»Ÿæœªåˆå§‹åŒ–",
                "message": "ç³»ç»Ÿå¯åŠ¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
            }), 503
        
        # è·å–é—®é¢˜
        data = request.get_json()
        if not data or 'question' not in data:
            return jsonify({
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ",
                "message": "è¯·åœ¨è¯·æ±‚ä½“ä¸­æä¾› 'question' å­—æ®µ",
                "example": {"question": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"}
            }), 400
        
        question = data['question']
        
        if not question.strip():
            return jsonify({
                "error": "é—®é¢˜ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # æ‰§è¡Œé—®ç­”
        print(f"â“ æ”¶åˆ°é—®é¢˜: {question}")
        result = qa_chain({"query": question})
        print(f"âœ… å›ç­”å·²ç”Ÿæˆ")
        
        # æå–æºæ–‡æ¡£
        sources = []
        if 'source_documents' in result:
            for i, doc in enumerate(result['source_documents'], 1):
                sources.append({
                    "id": i,
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                })
        
        return jsonify({
            "question": question,
            "answer": result['result'],
            "source_documents": sources,
            "sources_count": len(sources)
        })
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return jsonify({
            "error": "å¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯",
            "message": str(e)
        }), 500


# æ¨¡å—åŠ è½½æ—¶ç«‹å³åˆå§‹åŒ–ï¼ˆGunicorn ä¹Ÿä¼šæ‰§è¡Œï¼‰
try:
    initialize_qa_system()
except Exception as e:
    print(f"\nåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    print("å®¹å™¨å°†å¯åŠ¨ä½† QA åŠŸèƒ½ä¸å¯ç”¨\n")


if __name__ == '__main__':
    # å¯åŠ¨ Flask åº”ç”¨ï¼ˆä»…ç”¨äºæœ¬åœ°æµ‹è¯•ï¼‰
    port = int(os.getenv('PORT', 8080))
    print(f"å¯åŠ¨ Flask æœåŠ¡å™¨ï¼Œç«¯å£: {port}")
    app.run(host='0.0.0.0', port=port, debug=False)
