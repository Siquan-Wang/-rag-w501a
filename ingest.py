"""
æ•°æ®æ‘„å…¥è„šæœ¬ - å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åˆ° FAISS
"""
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

# é…ç½®
DATA_FILE = "data.txt"
FAISS_INDEX_PATH = "faiss_index"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def create_vector_store():
    """åˆ›å»ºå‘é‡å­˜å‚¨"""
    print("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®...")
    
    # æ£€æŸ¥ OpenAI API Key
    if not OPENAI_API_KEY:
        raise ValueError("âŒ æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(DATA_FILE):
        raise FileNotFoundError(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {DATA_FILE}")
    
    # 1. åŠ è½½æ–‡æ¡£
    print(f"ğŸ“– æ­£åœ¨åŠ è½½æ–‡æ¡£: {DATA_FILE}")
    loader = TextLoader(DATA_FILE, encoding='utf-8')
    documents = loader.load()
    print(f"âœ… åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆ†å‰²æ–‡æœ¬
    print("âœ‚ï¸  æ­£åœ¨åˆ†å‰²æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    texts = text_splitter.split_documents(documents)
    print(f"âœ… åˆ†å‰²æˆ {len(texts)} ä¸ªæ–‡æœ¬å—")
    
    # 3. åˆ›å»º embeddings
    print("ğŸ”® æ­£åœ¨åˆ›å»ºå‘é‡åµŒå…¥...")
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
    
    # 4. åˆ›å»ºå¹¶ä¿å­˜ FAISS å‘é‡å­˜å‚¨
    print("ğŸ’¾ æ­£åœ¨åˆ›å»º FAISS å‘é‡å­˜å‚¨...")
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    # ä¿å­˜åˆ°æœ¬åœ°
    vectorstore.save_local(FAISS_INDEX_PATH)
    print(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜åˆ°: {FAISS_INDEX_PATH}")
    
    # 5. æµ‹è¯•æ£€ç´¢
    print("\nğŸ§ª æµ‹è¯•å‘é‡æ£€ç´¢...")
    query = "è¿™æ˜¯ä»€ä¹ˆç³»ç»Ÿï¼Ÿ"
    docs = vectorstore.similarity_search(query, k=2)
    print(f"æŸ¥è¯¢: {query}")
    print(f"æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£:")
    for i, doc in enumerate(docs, 1):
        print(f"\næ–‡æ¡£ {i}:")
        print(doc.page_content[:200] + "...")
    
    print("\nâœ¨ æ•°æ®å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    try:
        create_vector_store()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        exit(1)

